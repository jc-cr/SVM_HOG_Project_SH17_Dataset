\section{Results}

\subsection{Evaluation Metrics}

The SH17 paper uses the following metrics to benchmark their model's performances:
\begin{equation}
Precision = \frac{True\ Positive}{True\ Positive + False\ Positive}
\end{equation}

\begin{equation}
Recall = \frac{True\ Positive}{True\ Positive + False\ Negative}
\end{equation}

The Mean Average Precision (mAP) metric is also used in the SH17 paper.
The mAP metric is calculated by averaging the average precision (AP) of each class, which in the COCO evaluation system \cite{cocodatasetpage} is calculated at different Intersection over Union (IoU) thresholds.

The Intersection over Union (IoU) metric is used to gauge the accuracy of object localization. 
It calculates the overlap between the ground truth bounding boxes ($b_g$) and the model's predicted bounding boxes ($b_{pred}$) as follows:
\begin{equation}
IoU = \frac{Area(b_{pred} \cap b_g)}{Area(b_{pred} \cup b_g)}
\end{equation}
where $b_g$ represents the ground truth bounding box, and $b_{pred}$ denotes the bounding box predicted by the model.


\subsection{Model Performance}

The results of the developed model and the YOLO models trained in SH-17 paper are displayed in Table \ref{tab:results}.
Our model is listed on the top row of the table as HOG-SVM, and the YOLO models are listed below it.
The top performing model is bolded.

\begin{table}[H]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}  % Reduce column spacing
        \begin{tabularx}{\textwidth}{|X|c|c|c|c|c|c|c|}  % Note the X column type
        \hline
            \makecell[l]{Model} & 
            \makecell{Params\\(M)} & 
            \makecell{Images} & 
            \makecell{Instances} & 
            \makecell{P\\(\%)} & 
            \makecell{R\\(\%)} & 
            \makecell{mAP50\\(\%)} & 
            \makecell{mAP50-95\\(\%)} \\ \hline
        HOG-SVM & - & 1620 & - & 3.4 & 1.9 & 0.4 & 0.2 \\ \hline
        Yolo-8-n & 3.2 & 1620 & 15358 & 67.5 & 53.6 & 58.0 & 36.6 \\ \hline
        Yolo-8-s & 11.2 & 1620 & 15358 & 81.5 & 55.7 & 63.7 & 41.7 \\ \hline
        Yolo-8-m & 25.9 & 1620 & 15358 & 77.1 & 60.5 & 66.6 & 45.7 \\ \hline
        Yolo-8-l & 43.7 & 1620 & 15358 & 76.7 & 62.9 & 68.0 & 47.0 \\ \hline
        Yolo-8-x & 68.2 & 1620 & 15358 & 77.1 & 63.1 & 69.3 & 47.2 \\ \hline
        Yolo-9-t & 2.0 & 1620 & 15358 & 75.0 & 52.6 & 58.5 & 37.5 \\ \hline
        Yolo-9-s & 7.2 & 1620 & 15358 & 73.6 & 60.2 & 65.3 & 42.9 \\ \hline
        Yolo-9-m & 20.1 & 1620 & 15358 & 77.4 & 62.0 & 68.6 & 46.5 \\ \hline
        Yolo-9-c & 25.5 & 1620 & 15358 & 79.6 & 60.8 & 67.7 & 46.5 \\ \hline
        {\bfseries Yolo-9-e} & {\bfseries 58.1} & {\bfseries 1620} & {\bfseries 15358} & {\bfseries 81.0} & {\bfseries 65.0} & {\bfseries 70.9} & {\bfseries 48.7} \\ \hline
        Yolo-10-n & 2.3 & 1620 & 15358 & 66.8 & 53.2 & 57.2 & 35.9 \\ \hline
        Yolo-10-s & 7.2 & 1620 & 15358 & 75.8 & 57.0 & 62.7 & 40.9 \\ \hline
        Yolo-10-m & 15.4 & 1620 & 15358 & 71.4 & 61.4 & 65.7 & 43.8 \\ \hline
        Yolo-10-b & 19.1 & 1620 & 15358 & 77.7 & 59.1 & 65.8 & 45.1 \\ \hline
        Yolo-10-l & 24.4 & 1620 & 15358 & 76.0 & 61.8 & 67.4 & 46.0 \\ \hline
        Yolo-10-x & 29.5 & 1620 & 15358 & 76.8 & 62.8 & 67.8 & 46.7 \\ \hline
    \end{tabularx}
    \caption{Model performance comparison} 
    \label{tab:results}
\end{table}


\subsection{Discussion}

From evaluating a few samples of the model's predictions, it was observed that our model would fall below the 0.5 IoU threshold, which is the minimum threshold for a detection to be considered a true positive, thus the model's performance was extremely poor on using COCO evaluation metrics.

As described in the previous section, steps were taken in the implementation to merge the multiple prediction boxes into a more representative single box per person class, which would potentially improve the IoU value with respect to the ground truth boxes.
In improving the IoU value, we could see more true positives being detected, which would improve the precision and recall values of the model.
In addition hyper parameter tuning for the SGD optimizer could be done in training, but these improvements are beyond the scope of this project.
