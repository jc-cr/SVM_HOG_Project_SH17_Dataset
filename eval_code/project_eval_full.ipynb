{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccc2661-99e2-4f9e-842a-daacb90b6995",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9df48a-9e2c-4bd2-be13-9b482d3aba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skimage.feature import hog\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be5745-a99e-4cb6-86b7-facc3ac32204",
   "metadata": {},
   "source": [
    "## Eval Metrics Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18c3ee6-4bfd-42a3-b7c0-02e5502aeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalMetrics:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictions = None\n",
    "        self.ground_truths = None\n",
    "        self.iou_thresholds = None\n",
    "        \n",
    "    def calculate_ap(self, precision, recall):\n",
    "        \"\"\"Calculate Average Precision using 11-point interpolation\"\"\"\n",
    "        if len(precision) == 0 or len(recall) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        ap = 0\n",
    "        for t in np.arange(0, 1.1, 0.1):\n",
    "            mask = recall >= t\n",
    "            if not np.any(mask):\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(precision[mask])\n",
    "            ap += p / 11\n",
    "        return ap\n",
    "    \n",
    "\n",
    "    def print_metrics(self, metrics):\n",
    "        \"\"\"Print evaluation metrics\"\"\"\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Precision: {metrics['precision']:.1f}%\")\n",
    "        print(f\"Recall: {metrics['recall']:.1f}%\")\n",
    "        print(f\"mAP@50: {metrics['mAP50']:.1f}%\")\n",
    "        print(f\"mAP@50-95: {metrics['mAP50-95']:.1f}%\")\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_iou(self, box1, box2, debug=False):\n",
    "        \"\"\"Calculate IoU between two boxes with enhanced precision and debugging\"\"\"\n",
    "        # Convert all coordinates to float64 for better precision\n",
    "        box1 = np.array(box1, dtype=np.float64)\n",
    "        box2 = np.array(box2, dtype=np.float64)\n",
    "        \n",
    "        # Get intersection coordinates\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "    \n",
    "        # Calculate areas\n",
    "        w = max(0.0, x2 - x1)\n",
    "        h = max(0.0, y2 - y1)\n",
    "        intersection = w * h\n",
    "        \n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = float(box1_area + box2_area - intersection)\n",
    "        \n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        # Single pass of precision-recall calculation\n",
    "        all_tps = []\n",
    "        all_fps = []\n",
    "        all_scores = []\n",
    "        total_gt = 0\n",
    "        \n",
    "        # Process each image once\n",
    "        for img_idx, (pred_boxes, gt_boxes) in enumerate(zip(self.predictions, self.ground_truths)):\n",
    "            total_gt += len(gt_boxes)\n",
    "            if len(pred_boxes) == 0:\n",
    "                continue\n",
    "                \n",
    "            pred_boxes = np.array(pred_boxes, dtype=np.float64)\n",
    "            gt_boxes = np.array(gt_boxes, dtype=np.float64)\n",
    "            \n",
    "            # Track matches at each threshold\n",
    "            thresholds_tps = np.zeros((len(self.iou_thresholds), len(pred_boxes)))\n",
    "            thresholds_fps = np.zeros((len(self.iou_thresholds), len(pred_boxes)))\n",
    "            \n",
    "            # Sort by confidence\n",
    "            scores = pred_boxes[:, 4]\n",
    "            sort_idx = np.argsort(-scores)\n",
    "            pred_boxes = pred_boxes[sort_idx]\n",
    "            \n",
    "            # For each prediction\n",
    "            for pred_idx, pred in enumerate(pred_boxes):\n",
    "                # Track best match at each threshold\n",
    "                best_ious = np.zeros(len(self.iou_thresholds))\n",
    "                matched_gt_indices = [-1] * len(self.iou_thresholds)\n",
    "                \n",
    "                # Compare with each ground truth\n",
    "                for gt_idx, gt in enumerate(gt_boxes):\n",
    "                    iou = self.calculate_iou(pred[:4], gt)\n",
    "                    \n",
    "                    # Update if better match found\n",
    "                    for thresh_idx, _ in enumerate(self.iou_thresholds):\n",
    "                        if iou > best_ious[thresh_idx]:\n",
    "                            best_ious[thresh_idx] = iou\n",
    "                            matched_gt_indices[thresh_idx] = gt_idx\n",
    "                \n",
    "                # Record matches meeting thresholds\n",
    "                for thresh_idx, thresh in enumerate(self.iou_thresholds):\n",
    "                    if best_ious[thresh_idx] >= thresh:\n",
    "                        thresholds_tps[thresh_idx, pred_idx] = 1\n",
    "                    else:\n",
    "                        thresholds_fps[thresh_idx, pred_idx] = 1\n",
    "                \n",
    "                all_scores.append(pred[4])\n",
    "            \n",
    "            all_tps.append(thresholds_tps)\n",
    "            all_fps.append(thresholds_fps)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        if len(all_tps) > 0:\n",
    "            all_tps = np.concatenate(all_tps, axis=1)\n",
    "            all_fps = np.concatenate(all_fps, axis=1)\n",
    "            \n",
    "            # Calculate AP for each threshold\n",
    "            aps = []\n",
    "            for thresh_idx in range(len(self.iou_thresholds)):\n",
    "                cum_tp = np.cumsum(all_tps[thresh_idx])\n",
    "                cum_fp = np.cumsum(all_fps[thresh_idx])\n",
    "                \n",
    "                # Calculate precision and recall\n",
    "                recall = cum_tp / total_gt if total_gt > 0 else np.zeros_like(cum_tp)\n",
    "                precision = cum_tp / (cum_tp + cum_fp)\n",
    "                \n",
    "                # Calculate AP\n",
    "                ap = self.calculate_ap(precision, recall)\n",
    "                aps.append(ap)\n",
    "            \n",
    "            # Final metrics as per paper\n",
    "            metrics['mAP50'] = aps[0] * 100  # AP at IoU=0.5\n",
    "            metrics['mAP50-95'] = np.mean(aps) * 100  # Mean AP across thresholds\n",
    "            \n",
    "            # Use IoU=0.5 threshold for precision/recall\n",
    "            cum_tp = np.cumsum(all_tps[0])\n",
    "            cum_fp = np.cumsum(all_fps[0])\n",
    "            recall = cum_tp / total_gt if total_gt > 0 else np.zeros_like(cum_tp)\n",
    "            precision = cum_tp / (cum_tp + cum_fp)\n",
    "            metrics['precision'] = np.mean(precision) * 100\n",
    "            metrics['recall'] = np.mean(recall) * 100\n",
    "            \n",
    "        else:\n",
    "            metrics = {\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'mAP50': 0.0,\n",
    "                'mAP50-95': 0.0\n",
    "            }\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bdad2-0a03-44e1-8103-1281177debfd",
   "metadata": {},
   "source": [
    "## Detection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603321d2-9d8d-40a6-a4f3-566864eaa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOGPersonDetectorEval:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"Initialize evaluator with trained model\"\"\"\n",
    "        self.eval = EvalMetrics()\n",
    "        self.clf = joblib.load(model_path)\n",
    "        \n",
    "        self.predictions = []\n",
    "        self.ground_truths = []\n",
    "        self.iou_thresholds = np.linspace(0.5, 0.95, 10)  # For mAP50-95\n",
    "        \n",
    "        # Add HOG parameters\n",
    "        self.hog_params = {\n",
    "            'orientations': 9,\n",
    "            'pixels_per_cell': (8, 8),\n",
    "            'cells_per_block': (2, 2),\n",
    "            'block_norm': 'L2'\n",
    "        }\n",
    " \n",
    "    def visualize_results(self, image, gt_boxes, pred_boxes, save_path):\n",
    "        \"\"\"Visualize ground truth and predictions on image\"\"\"\n",
    "        try:\n",
    "            vis_img = image.copy()\n",
    "            \n",
    "            # Draw ground truth boxes in green\n",
    "            for box in gt_boxes:\n",
    "                cv2.rectangle(vis_img, (box[0], box[1]), (box[2], box[3]), \n",
    "                             (0, 255, 0), 2)\n",
    "                cv2.putText(vis_img, 'GT', (box[0], box[1]-5),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw predicted boxes in red\n",
    "            for box in pred_boxes:\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                conf = box[4]\n",
    "                \n",
    "                # Draw box\n",
    "                cv2.rectangle(vis_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                \n",
    "                # Add confidence score in better position\n",
    "                conf_text = f\"{conf:.2f}\"\n",
    "                text_size = cv2.getTextSize(conf_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                # Position text to be visible\n",
    "                text_x = x1\n",
    "                text_y = y1 - 10 if y1 > 20 else y2 + 20\n",
    "                # Add background rectangle for text\n",
    "                cv2.rectangle(vis_img, \n",
    "                             (text_x, text_y - text_size[1] - 4),\n",
    "                             (text_x + text_size[0], text_y + 4),\n",
    "                             (0, 0, 0), -1)\n",
    "                # Add text\n",
    "                cv2.putText(vis_img, conf_text, (text_x, text_y),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "            # Add image info\n",
    "            h, w = image.shape[:2]\n",
    "            info_text = f\"Image size: {w}x{h}, Detections: {len(pred_boxes)}\"\n",
    "            cv2.putText(vis_img, info_text, (10, 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "            # Save image\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            cv2.imwrite(save_path, vis_img)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in visualization: {e}\")\n",
    "        \n",
    "            \n",
    "    def calculate_box_overlap_ratio(self, box1, box2):\n",
    "        \"\"\"Calculate overlap ratio relative to smaller box\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        \n",
    "        # Use area of smaller box as denominator\n",
    "        smaller_area = min(box1_area, box2_area)\n",
    "        return intersection / smaller_area\n",
    "    \n",
    "\n",
    "        \n",
    "    def load_image_list(self, split_file_path):\n",
    "        \"\"\"Load image list from train/val txt file\"\"\"\n",
    "        with open(split_file_path) as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "\n",
    "   \n",
    "    def extract_hog_features(self, image, target_size=(128, 64)):\n",
    "        \"\"\"Extract HOG features from an image patch\"\"\"\n",
    "        if image is None or image.size == 0:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            resized = cv2.resize(image, target_size)\n",
    "            features = hog(\n",
    "                resized,\n",
    "                orientations=self.hog_params['orientations'],\n",
    "                pixels_per_cell=self.hog_params['pixels_per_cell'],\n",
    "                cells_per_block=self.hog_params['cells_per_block'],\n",
    "                block_norm=self.hog_params['block_norm'],\n",
    "                feature_vector=True\n",
    "            )\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting HOG features: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_person_boxes(self, xml_path):\n",
    "        \"\"\"Extract person bounding boxes from XML annotation\"\"\"\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            if obj.find('name').text.lower() == 'person':\n",
    "                bbox = obj.find('bndbox')\n",
    "                xmin = int(float(bbox.find('xmin').text))\n",
    "                ymin = int(float(bbox.find('ymin').text))\n",
    "                xmax = int(float(bbox.find('xmax').text))\n",
    "                ymax = int(float(bbox.find('ymax').text))\n",
    "                boxes.append((xmin, ymin, xmax, ymax))\n",
    "        return boxes\n",
    "        \n",
    "    def non_max_suppression(self, boxes, overlap_thresh):\n",
    "        \"\"\"Apply non-maximum suppression to avoid duplicate detections\"\"\"\n",
    "        if len(boxes) == 0:\n",
    "            return []\n",
    "            \n",
    "        # Convert to numpy array if needed\n",
    "        if not isinstance(boxes, np.ndarray):\n",
    "            boxes = np.array(boxes)\n",
    "        \n",
    "        # Get coordinates and scores\n",
    "        x1 = boxes[:, 0]\n",
    "        y1 = boxes[:, 1]\n",
    "        x2 = boxes[:, 2]\n",
    "        y2 = boxes[:, 3]\n",
    "        scores = boxes[:, 4]\n",
    "        \n",
    "        # Calculate areas\n",
    "        areas = (x2 - x1) * (y2 - y1)\n",
    "        \n",
    "        # Sort by confidence score\n",
    "        order = scores.argsort()[::-1]\n",
    "        \n",
    "        keep = []\n",
    "        while order.size > 0:\n",
    "            i = order[0]\n",
    "            keep.append(i)\n",
    "            \n",
    "            if order.size == 1:\n",
    "                break\n",
    "                \n",
    "            # Get overlaps\n",
    "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "            \n",
    "            w = np.maximum(0.0, xx2 - xx1)\n",
    "            h = np.maximum(0.0, yy2 - yy1)\n",
    "            inter = w * h\n",
    "            \n",
    "            # Calculate IoU\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "            \n",
    "            # Get indices of boxes to keep\n",
    "            inds = np.where(ovr <= overlap_thresh)[0]\n",
    "            order = order[inds + 1]\n",
    "        \n",
    "        return keep\n",
    "    \n",
    "\n",
    "\n",
    "    def detect_persons(self, image, conf_thresh=0.5, max_post_processing_steps=1, merge_thresh=0.3, nms_thresh=0.5):\n",
    "        \"\"\"Detect persons in image with sliding window\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "        # Resize large images\n",
    "        max_dim = 1000\n",
    "        scale_factor = 1.0\n",
    "        if max(image.shape) > max_dim:\n",
    "            scale_factor = max_dim / max(image.shape)\n",
    "            image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor)\n",
    "        \n",
    "        detections = []  # Keep as list initially\n",
    "        window_size = (64, 128)\n",
    "        stride = 32\n",
    "        scales = [0.5, 0.75, 1.0, 1.25]\n",
    "    \n",
    "        try:\n",
    "            for scale in scales:\n",
    "                scaled = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "                \n",
    "                y_steps = range(0, scaled.shape[0] - window_size[1], stride)\n",
    "                x_steps = range(0, scaled.shape[1] - window_size[0], stride)\n",
    "                \n",
    "                batch_size = 64\n",
    "                windows = []\n",
    "                coordinates = []\n",
    "                \n",
    "                for y in y_steps:\n",
    "                    for x in x_steps:\n",
    "                        windows.append(scaled[y:y + window_size[1], x:x + window_size[0]])\n",
    "                        coordinates.append((x, y))\n",
    "                        \n",
    "                        if len(windows) >= batch_size:\n",
    "                            batch_features = [self.extract_hog_features(win, target_size=window_size) for win in windows]\n",
    "                            batch_features = [f for f in batch_features if f is not None]\n",
    "                            \n",
    "                            if batch_features:\n",
    "                                # Use decision_function score directly since StandardScaler is in pipeline\n",
    "                                confidences = self.clf.decision_function(batch_features)\n",
    "                                \n",
    "                                # Add detections above threshold\n",
    "                                for (x, y), confidence in zip(coordinates, confidences):\n",
    "                                    if confidence > conf_thresh:\n",
    "                                        x1 = int(x / scale / scale_factor)\n",
    "                                        y1 = int(y / scale / scale_factor)\n",
    "                                        x2 = int((x + window_size[0]) / scale / scale_factor)\n",
    "                                        y2 = int((y + window_size[1]) / scale / scale_factor)\n",
    "                                        detections.append([x1, y1, x2, y2, float(confidence)])\n",
    "                            \n",
    "                            windows = []\n",
    "                            coordinates = []\n",
    "                \n",
    "                # Process remaining windows\n",
    "                if windows:\n",
    "                    batch_features = [self.extract_hog_features(win, target_size=window_size) for win in windows]\n",
    "                    batch_features = [f for f in batch_features if f is not None]\n",
    "                    \n",
    "                    if batch_features:\n",
    "                        confidences = self.clf.decision_function(batch_features)\n",
    "                        \n",
    "                        for (x, y), confidence in zip(coordinates, confidences):\n",
    "                            if confidence > conf_thresh:\n",
    "                                x1 = int(x / scale / scale_factor)\n",
    "                                y1 = int(y / scale / scale_factor)\n",
    "                                x2 = int((x + window_size[0]) / scale / scale_factor)\n",
    "                                y2 = int((y + window_size[1]) / scale / scale_factor)\n",
    "                                detections.append([x1, y1, x2, y2, float(confidence)])\n",
    "            \n",
    "            # Post-process detections\n",
    "            if len(detections) > 0:\n",
    "                detections = np.array(detections)\n",
    "                \n",
    "                # Track number of boxes before and after post-processing\n",
    "                prev_num_boxes = -1\n",
    "                for step in range(max_post_processing_steps):\n",
    "                 \n",
    "                    \n",
    "                    curr_num_boxes = len(detections) if detections is not None else 0\n",
    "                    if curr_num_boxes == prev_num_boxes:\n",
    "                        break\n",
    "                    \n",
    "                    prev_num_boxes = curr_num_boxes\n",
    "                    \n",
    "                    # First adjust aspect ratios and merge/NMS\n",
    "                    detections = self.post_process_detection(detections, merge_thresh, nms_thresh) \n",
    "                    if detections is None or len(detections) == 0:\n",
    "                        break\n",
    "                        \n",
    "                return detections\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in detect_persons: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def evaluate(self, val_file_path, val_images_path, annotations_path, \n",
    "                visualization_path=None, conf_thresh=0.5, max_post_processing_steps=3,\n",
    "                merge_thresh=0.3, nms_thresh=0.5, max_samples=None):\n",
    "        \"\"\"\n",
    "        Evaluate model on validation set\n",
    "        \n",
    "        Args:\n",
    "            val_file_path: Path to validation image list\n",
    "            val_images_path: Path to validation images\n",
    "            annotations_path: Path to annotation files\n",
    "            visualization_path: Path to save visualizations\n",
    "            conf_thresh: Confidence threshold for detections (default: 0.5)\n",
    "            max_post_processing_steps: Number of post-processing iterations (default: 3)\n",
    "            merge_thresh: Threshold for merging overlapping boxes (default: 0.3) \n",
    "            nms_thresh: Threshold for NMS (default: 0.5)\n",
    "            max_samples: Maximum number of images to evaluate (default: None, evaluates all)\n",
    "        \"\"\"\n",
    "        print(\"\\nEvaluation Parameters:\")\n",
    "        print(f\"- Confidence threshold: {conf_thresh}\")\n",
    "        print(f\"- Max post-processing steps: {max_post_processing_steps}\")\n",
    "        print(f\"- Merge threshold: {merge_thresh}\")\n",
    "        print(f\"- NMS threshold: {nms_thresh}\")\n",
    "        print(f\"- Max samples: {max_samples if max_samples else 'All'}\")\n",
    "        print(f\"- HOG Parameters: {self.hog_params}\")\n",
    "        print(\"\\nStarting evaluation...\")\n",
    "        \n",
    "        # Load validation images\n",
    "        val_images = self.load_image_list(val_file_path)\n",
    "        if max_samples:\n",
    "            val_images = val_images[:max_samples]\n",
    "        print(f\"Evaluating on {len(val_images)} images...\")\n",
    "        \n",
    "        # Create visualization directory if needed\n",
    "        if visualization_path and not os.path.exists(visualization_path):\n",
    "            os.makedirs(visualization_path)\n",
    "        \n",
    "        # Initialize progress tracking\n",
    "        processed_images = 0\n",
    "        skipped_images = 0\n",
    "        \n",
    "        # Reset predictions and ground truths\n",
    "        self.predictions = []\n",
    "        self.ground_truths = []\n",
    "        \n",
    "        # Process each image\n",
    "        for i, img_file in enumerate(tqdm(val_images)):\n",
    "            try:\n",
    "                img_path = os.path.join(val_images_path, img_file)\n",
    "                xml_path = os.path.join(annotations_path, \n",
    "                                      os.path.splitext(img_file)[0] + '.xml')\n",
    "                \n",
    "                if not os.path.exists(img_path) or not os.path.exists(xml_path):\n",
    "                    skipped_images += 1\n",
    "                    continue\n",
    "                    \n",
    "                # Load and process image\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    skipped_images += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get ground truth boxes\n",
    "                gt_boxes = self.get_person_boxes(xml_path)\n",
    "                \n",
    "                # Detect persons\n",
    "                detections = self.detect_persons(\n",
    "                    image, \n",
    "                    conf_thresh=conf_thresh,\n",
    "                    max_post_processing_steps=max_post_processing_steps,\n",
    "                    merge_thresh=merge_thresh,\n",
    "                    nms_thresh=nms_thresh\n",
    "                )\n",
    "                \n",
    "                # Convert detections to list if numpy array\n",
    "                if isinstance(detections, np.ndarray):\n",
    "                    detections = detections.tolist()\n",
    "                \n",
    "                self.ground_truths.append(gt_boxes)\n",
    "                self.predictions.append(detections if detections is not None else [])\n",
    "                \n",
    "\n",
    "                if visualization_path and i < 5:\n",
    "                    viz_path = os.path.join(visualization_path, f\"eval_{i}.jpg\")\n",
    "                    self.visualize_results(image, gt_boxes, detections, viz_path)\n",
    "                    \n",
    "                    # Add HOG visualizations\n",
    "                    if len(detections) > 0:\n",
    "                        hog_viz_path = os.path.join(visualization_path, f\"eval_{i}_hog.jpg\")\n",
    "                        self.visualize_detection_with_hog(image, detections[0], hog_viz_path)\n",
    "                        \n",
    "                    windows_viz_path = os.path.join(visualization_path, f\"eval_{i}_windows.jpg\")\n",
    "                    self.visualize_sliding_windows_with_hog(image, save_path=windows_viz_path)\n",
    "                                    \n",
    "                \n",
    "                processed_images += 1\n",
    "                \n",
    "      \n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing image {img_file}: {e}\")\n",
    "                skipped_images += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nEvaluation complete:\")\n",
    "        print(f\"- Total processed: {processed_images}\")\n",
    "        print(f\"- Total skipped: {skipped_images}\")\n",
    "        \n",
    "        # Set evaluation metrics object attributes\n",
    "        self.eval.predictions = self.predictions\n",
    "        self.eval.ground_truths = self.ground_truths\n",
    "        self.eval.iou_thresholds = self.iou_thresholds\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.eval.calculate_metrics()\n",
    "        self.eval.print_metrics(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_detection_with_hog(self, image, detection, save_path=None):\n",
    "        \"\"\"Visualize HOG features for a detected person\"\"\"\n",
    "        if len(detection) < 4:\n",
    "            print(\"Invalid detection format\")\n",
    "            return\n",
    "            \n",
    "        # Extract person window\n",
    "        x1, y1, x2, y2 = map(int, detection[:4])\n",
    "        person_window = image[y1:y2, x1:x2]\n",
    "        \n",
    "        if person_window.size == 0:\n",
    "            print(\"Invalid window size\")\n",
    "            return\n",
    "        \n",
    "        # Create visualization with 3 subplots\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        \n",
    "        # Original detection\n",
    "        ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax1.add_patch(rect)\n",
    "        ax1.set_title('Detection')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Extracted window\n",
    "        ax2.imshow(cv2.cvtColor(person_window, cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title('Person Window')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # HOG visualization\n",
    "        _, hog_image = self.visualize_hog(person_window)\n",
    "        ax3.imshow(hog_image, cmap='gray')\n",
    "        ax3.set_title('HOG Features')\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def visualize_sliding_windows_with_hog(self, image, window_size=(64, 128), \n",
    "                                         stride=32, num_windows=3, save_path=None):\n",
    "        \"\"\"Visualize HOG features for random sliding windows\"\"\"\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "            \n",
    "        height, width = gray.shape\n",
    "        windows = []\n",
    "        \n",
    "        # Generate random window positions while ensuring windows fit within image\n",
    "        for _ in range(num_windows):\n",
    "            max_x = max(0, width - window_size[0] - 1)\n",
    "            max_y = max(0, height - window_size[1] - 1)\n",
    "            if max_x > 0 and max_y > 0:\n",
    "                x = np.random.randint(0, max_x)\n",
    "                y = np.random.randint(0, max_y)\n",
    "                windows.append((x, y))\n",
    "        \n",
    "        if not windows:\n",
    "            print(\"Image too small for selected window size\")\n",
    "            return\n",
    "            \n",
    "        # Create visualization grid\n",
    "        fig, axes = plt.subplots(len(windows), 3, figsize=(12, 4*len(windows)))\n",
    "        if len(windows) == 1:\n",
    "            axes = [axes]  # Make it indexable for single window case\n",
    "        \n",
    "        for i, (x, y) in enumerate(windows):\n",
    "            try:\n",
    "                # Extract window\n",
    "                window = gray[y:y + window_size[1], x:x + window_size[0]]\n",
    "                \n",
    "                # Original window\n",
    "                axes[i][0].imshow(window, cmap='gray')\n",
    "                axes[i][0].set_title(f'Window {i+1}')\n",
    "                axes[i][0].axis('off')\n",
    "                \n",
    "                # Resized window\n",
    "                resized = cv2.resize(window, window_size)\n",
    "                axes[i][1].imshow(resized, cmap='gray')\n",
    "                axes[i][1].set_title('Resized')\n",
    "                axes[i][1].axis('off')\n",
    "                \n",
    "                # HOG visualization\n",
    "                _, hog_image = self.visualize_hog(window)\n",
    "                axes[i][2].imshow(hog_image, cmap='gray')\n",
    "                axes[i][2].set_title('HOG Features')\n",
    "                axes[i][2].axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing window {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    def visualize_hog(self, image, window_size=(64, 128), save_path=None):\n",
    "        \"\"\"Visualize HOG features for a given image window\"\"\"\n",
    "        if image is None or image.size == 0:\n",
    "            print(\"Invalid image\")\n",
    "            return None, None\n",
    "            \n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "            \n",
    "        # Resize to window size\n",
    "        try:\n",
    "            resized = cv2.resize(gray, window_size)\n",
    "            \n",
    "            # Calculate HOG features with visualization\n",
    "            features, hog_image = hog(\n",
    "                resized,\n",
    "                orientations=self.hog_params['orientations'],\n",
    "                pixels_per_cell=self.hog_params['pixels_per_cell'],\n",
    "                cells_per_block=self.hog_params['cells_per_block'],\n",
    "                block_norm=self.hog_params['block_norm'],\n",
    "                visualize=True\n",
    "            )\n",
    "            \n",
    "            if save_path:\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "                \n",
    "                # Original image\n",
    "                ax1.imshow(resized, cmap='gray')\n",
    "                ax1.set_title('Original Window')\n",
    "                ax1.axis('off')\n",
    "                \n",
    "                # HOG visualization\n",
    "                ax2.imshow(hog_image, cmap='gray')\n",
    "                ax2.set_title('HOG Features')\n",
    "                ax2.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(save_path)\n",
    "                plt.close()\n",
    "                \n",
    "            return features, hog_image\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in visualize_hog: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    \n",
    "    def post_process_detection(self, detections, merge_thresh=0.3, nms_thresh=0.5):\n",
    "        \"\"\"Enhanced iterative post-processing with progressive merging\"\"\"\n",
    "        if len(detections) == 0:\n",
    "            return detections\n",
    "                \n",
    "        detections = np.array(detections)\n",
    "        \n",
    "        def create_density_clusters(boxes, radius=150):  # Increased radius\n",
    "            \"\"\"Group boxes based on density-based clustering with larger radius\"\"\"\n",
    "            centers = []\n",
    "            for box in boxes:\n",
    "                centers.append([\n",
    "                    (box[0] + box[2])/2,  # center x\n",
    "                    (box[1] + box[3])/2   # center y\n",
    "                ])\n",
    "            centers = np.array(centers)\n",
    "            \n",
    "            clusters = []\n",
    "            used = set()\n",
    "            \n",
    "            # Sort boxes by area to prioritize larger boxes\n",
    "            areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "            order = np.argsort(-areas)\n",
    "            \n",
    "            for i in order:\n",
    "                if i in used:\n",
    "                    continue\n",
    "                    \n",
    "                cluster = []\n",
    "                points_to_check = [i]\n",
    "                \n",
    "                while points_to_check:\n",
    "                    current = points_to_check.pop(0)\n",
    "                    if current in used:\n",
    "                        continue\n",
    "                        \n",
    "                    used.add(current)\n",
    "                    cluster.append(current)\n",
    "                    \n",
    "                    # Get current box dimensions\n",
    "                    curr_box = boxes[current]\n",
    "                    curr_width = curr_box[2] - curr_box[0]\n",
    "                    curr_height = curr_box[3] - curr_box[1]\n",
    "                    \n",
    "                    # Adaptive radius based on box size\n",
    "                    adaptive_radius = max(radius, max(curr_width, curr_height) * 0.5)\n",
    "                    \n",
    "                    # Find neighbors\n",
    "                    for j in range(len(boxes)):\n",
    "                        if j in used:\n",
    "                            continue\n",
    "                            \n",
    "                        # Calculate center distance\n",
    "                        dist = np.sqrt(np.sum((centers[current] - centers[j])**2))\n",
    "                        \n",
    "                        # Check if boxes overlap or are close\n",
    "                        if dist < adaptive_radius:\n",
    "                            points_to_check.append(j)\n",
    "                        else:\n",
    "                            # Also check if boxes overlap significantly\n",
    "                            iou = self.eval.calculate_iou(boxes[current], boxes[j])\n",
    "                            if iou > merge_thresh * 0.5:  # More lenient IOU threshold\n",
    "                                points_to_check.append(j)\n",
    "                \n",
    "                if cluster:\n",
    "                    clusters.append(cluster)\n",
    "            \n",
    "            return clusters\n",
    "    \n",
    "        def merge_cluster(boxes):\n",
    "            \"\"\"Merge boxes with enhanced size preservation and coordinate clamping\"\"\"\n",
    "            if len(boxes) == 0:\n",
    "                return None\n",
    "                \n",
    "            # Get box centers and confidences\n",
    "            centers = np.array([[\n",
    "                (box[0] + box[2])/2,\n",
    "                (box[1] + box[3])/2\n",
    "            ] for box in boxes])\n",
    "            \n",
    "            # Calculate density-based weights\n",
    "            densities = np.zeros(len(centers))\n",
    "            for i, center in enumerate(centers):\n",
    "                distances = np.sqrt(np.sum((centers - center)**2, axis=1))\n",
    "                densities[i] = np.sum(np.exp(-distances/100))\n",
    "            \n",
    "            weights = densities * boxes[:, 4]\n",
    "            weights = weights / np.sum(weights)\n",
    "            \n",
    "            # Get weighted center\n",
    "            center_x = np.average((boxes[:, 0] + boxes[:, 2])/2, weights=weights)\n",
    "            center_y = np.average((boxes[:, 1] + boxes[:, 3])/2, weights=weights)\n",
    "            \n",
    "            # Calculate dimensions with clamped margins\n",
    "            margin = 0.1\n",
    "            width = np.max(boxes[:, 2]) - np.min(boxes[:, 0])\n",
    "            height = np.max(boxes[:, 3]) - np.min(boxes[:, 1])\n",
    "            \n",
    "            margin_x = margin * width\n",
    "            margin_y = margin * height\n",
    "            \n",
    "            x1 = max(0, np.min(boxes[:, 0]) - margin_x)\n",
    "            y1 = max(0, np.min(boxes[:, 1]) - margin_y)\n",
    "            x2 = np.max(boxes[:, 2]) + margin_x\n",
    "            y2 = np.max(boxes[:, 3]) + margin_y\n",
    "            \n",
    "            # Adjust aspect ratio if needed\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            aspect_ratio = width / height\n",
    "            target_ratio = 0.41\n",
    "            \n",
    "            if aspect_ratio < target_ratio * 0.5:\n",
    "                width = height * target_ratio * 0.6\n",
    "                x1 = max(0, center_x - width/2)\n",
    "                x2 = center_x + width/2\n",
    "            elif aspect_ratio > target_ratio * 2.0:\n",
    "                height = width / (target_ratio * 1.5)\n",
    "                y1 = max(0, center_y - height/2)\n",
    "                y2 = center_y + height/2\n",
    "            \n",
    "            confidence = np.max(boxes[:, 4])\n",
    "            \n",
    "            return np.array([x1, y1, x2, y2, confidence])\n",
    "                 \n",
    "    \n",
    "        # Iterative merging process\n",
    "        prev_num_boxes = len(detections) + 1\n",
    "        \n",
    "        while len(detections) < prev_num_boxes:\n",
    "            prev_num_boxes = len(detections)\n",
    "            \n",
    "            # Create and merge clusters\n",
    "            clusters = create_density_clusters(detections)\n",
    "            final_boxes = []\n",
    "            \n",
    "            for cluster_indices in clusters:\n",
    "                cluster_boxes = detections[cluster_indices]\n",
    "                merged_box = merge_cluster(cluster_boxes)\n",
    "                if merged_box is not None:\n",
    "                    final_boxes.append(merged_box)\n",
    "            \n",
    "            if not final_boxes:\n",
    "                break\n",
    "                \n",
    "            detections = np.array(final_boxes)\n",
    "            \n",
    "            # Apply NMS\n",
    "            keep = self.non_max_suppression(detections, nms_thresh)\n",
    "            detections = detections[keep]\n",
    "            \n",
    "            # Break if no more merging possible\n",
    "            if len(detections) == prev_num_boxes:\n",
    "                break\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50345ed1-76b4-4dbb-91a2-68ec1df43b98",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6098bb-123f-4b1b-8932-e9f1b9a77b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Parameters:\n",
      "- Confidence threshold: 1000.0\n",
      "- Max post-processing steps: 2\n",
      "- Merge threshold: 0.3\n",
      "- NMS threshold: 0.5\n",
      "- Max samples: All\n",
      "- HOG Parameters: {'orientations': 9, 'pixels_per_cell': (8, 8), 'cells_per_block': (2, 2), 'block_norm': 'L2'}\n",
      "\n",
      "Starting evaluation...\n",
      "Evaluating on 1620 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1620/1620 [2:27:30<00:00,  5.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete:\n",
      "- Total processed: 1620\n",
      "- Total skipped: 0\n",
      "\n",
      "Evaluation Results:\n",
      "Precision: 3.4%\n",
      "Recall: 1.9%\n",
      "mAP@50: 0.4%\n",
      "mAP@50-95: 0.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = HOGPersonDetectorEval(\"model.pkl\")\n",
    "\n",
    "\n",
    "metrics = evaluator.evaluate(\n",
    "    val_file_path=\"dataset/val_files.txt\",\n",
    "    val_images_path=\"dataset/images\",\n",
    "    annotations_path=\"dataset/voc_labels\",\n",
    "    visualization_path=\"viz\",\n",
    "    conf_thresh=1000.0,\n",
    "    max_post_processing_steps=2,\n",
    "    merge_thresh=0.3,\n",
    "    nms_thresh=0.5,\n",
    "    max_samples=None\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
